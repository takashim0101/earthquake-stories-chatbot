# Node.js Server Configuration
# The port the backend server will listen on.
PORT=3001

# The URL of your React frontend. Used for Socket.IO and CORS configuration.
FRONTEND_URL=http://localhost:5173

# --- 1. Cloud LLM Configuration (For Real-time Chat in server.js) ---
# This is used by server.js for live chat responses.

# The API endpoint for the Gemini model you are using.
GEMINI_API_URL=https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent

# Your secret API key for the cloud service.
GEMINI_API_KEY=YOUR_SECRET_GEMINI_API_KEY_HERE


# --- 2. Local LLM Configuration (For Data Preprocessing in preprocess_data.py) ---
# This is used by preprocess_data.py to analyze stories locally using LM Studio.

# The API endpoint for the local LM Studio server.
# IMPORTANT: Ensure this IP (192.168.56.1) is accessible from where your Python script runs.
LM_STUDIO_API="http://192.168.56.1:1234/v1/chat/completions"

# The name of the model loaded in LM Studio.
MODEL="mistral-7b-instruct-v0.2"